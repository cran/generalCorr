% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/abs_stdapdC.R
\name{abs_stdapdC}
\alias{abs_stdapdC}
\title{Absolute values of gradients (apd's) of kernel regressions of x on y when
both x and y are standardized and control variables are present.}
\usage{
abs_stdapdC(x, y, ctrl)
}
\arguments{
\item{x}{{vector of data on the dependent variable}}

\item{y}{{data on the regressors which can be a matrix}}

\item{ctrl}{{Data matrix on the control variable(s) beyond causal path issues}}
}
\value{
Absolute values of kernel regression gradients are returned after
standardizing the data on both sides so that the magnitudes of amorphous
partial derivatives (apd's) are comparable between regression of x on y on
the one hand and regression of y on x on the other.
}
\description{
1) standardize the data to force mean zero and variance unity, 2) kernel
regress x on y and a matrix of control variables, 
with the option `gradients = TRUE' and finally 3) compute
the absolute values of gradients
}
\details{
The first argument is assumed to be the dependent variable.  If
\code{abs_stdapdC(x,y)} is used, you are regressing x on y (not the usual y
on x). The regressors can be a matrix with 2 or more columns. The missing values
are suitably ignored by the standardization.
}
\examples{
\dontrun{
set.seed(330)
x=sample(20:50)
y=sample(20:50)
z=sample(20:50)
abs_stdapdC(x,y,ctrl=z)
}
}
\seealso{
See  \code{\link{abs_stdapd}}.
}
\author{
Prof. H. D. Vinod, Economics Dept., Fordham University, NY
}
\concept{apd}
\concept{kernel regression gradients}
